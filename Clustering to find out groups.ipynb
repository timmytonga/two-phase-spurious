{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7be14ce",
   "metadata": {},
   "source": [
    "# Clustering for group info\n",
    "\n",
    "## TODO\n",
    "- First load the trained ERM model (should try highly regularized or not) \n",
    "- Then extract the features of the training examples (save into a file somewhere?) \n",
    "- Then perform k-means on the feature space of each label x correctlyclassified to obtain pseudo-group-labels \n",
    "- Then perform some algorithm that can take into account the group information for lowering robust loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd2ee704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from data.celebA_dataset import CelebADataset\n",
    "from models import model_attributes\n",
    "from data.dro_dataset import DRODataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# device='cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "root_dir = '/home/thiennguyen/research/datasets/celebA/'  # dir that contains data\n",
    "target_name= 'Blond_Hair'  # we are classifying whether the input image is blond or not\n",
    "confounder_names= ['Male']  # we aim to avoid learning spurious features... here it's the gender\n",
    "model_type= 'resnet10vw'  # what model we are using to process --> this is to determine the input size to rescale the image\n",
    "augment_data= False\n",
    "fraction=1.0\n",
    "splits = ['train', 'val', 'test']\n",
    "full_dataset = CelebADataset(root_dir=root_dir,\n",
    "        target_name=target_name,\n",
    "        confounder_names=confounder_names,\n",
    "        model_type=model_type,  # this string is to get the model's input size (for resizing) and input type (image or precomputed)\n",
    "        augment_data=augment_data)  # augment data adds random resized crop and random flip.\n",
    "\n",
    "subsets = full_dataset.get_splits(       # basically return the Subsets object with the appropriate indices for train/val/test\n",
    "        splits,                          # also implements subsampling --> just remove random indices of the appropriate groups in train\n",
    "        train_frac=fraction,   # fraction means how much of the train data to use --> randomly remove if less than 1\n",
    "        subsample_to_minority=False)\n",
    "\n",
    "dro_subsets = [  \n",
    "    DRODataset(\n",
    "        subsets[split],  # process each subset separately --> applying the transform parameter.\n",
    "        process_item_fn=None,\n",
    "        n_groups=full_dataset.n_groups,\n",
    "        n_classes=full_dataset.n_classes,\n",
    "        group_str_fn=full_dataset.group_str) \\\n",
    "    for split in splits]\n",
    "\n",
    "train_data, val_data, test_data = dro_subsets\n",
    "train_loader = train_data.get_loader(train=True, reweight_groups=False, batch_size=128)\n",
    "val_loader = val_data.get_loader(train=False, reweight_groups=None, batch_size=128)\n",
    "test_loader = test_data.get_loader(train=False, reweight_groups=None, batch_size=128)\n",
    "example_batch = next(iter(train_loader))\n",
    "example_batch = tuple(t.to(device) for t in example_batch)\n",
    "x, y, g, idxs = example_batch\n",
    "\n",
    "######################################################\n",
    "###########        Load models and optimizers.  ######\n",
    "######################################################\n",
    "from variable_width_resnet import resnet10vw\n",
    "\n",
    "n_classes = train_data.n_classes\n",
    "n_groups = train_data.n_groups\n",
    "margin_shape = n_classes\n",
    "\n",
    "resnet_width = 16\n",
    "lr = 0.001\n",
    "weight_decay = 0.001\n",
    "# load saved model\n",
    "log_path = 'logs/cluster_w16s0/'\n",
    "log_sr_path = 'logs/cluster_w16s0wd0.1/'\n",
    "model_sr_path = log_sr_path + 'joint/model_100.pth'\n",
    "model_path = log_path + 'joint/model_100.pth'\n",
    "\n",
    "modeln = resnet10vw(resnet_width, num_classes=margin_shape)\n",
    "modeln.load_state_dict(torch.load(model_path))\n",
    "modeln.to(device)\n",
    "\n",
    "model_sr = resnet10vw(resnet_width, num_classes=margin_shape)\n",
    "model_sr.load_state_dict(torch.load(model_sr_path))\n",
    "model_sr.to(device)\n",
    "\n",
    "\n",
    "def freeze_all_but_last_layer(model):\n",
    "    # freeze everything except the last layer\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "    # make sure that really worked\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    assert len(parameters) == 2  # fc.weight, fc.bias\n",
    "\n",
    "\n",
    "model = modeln\n",
    "\n",
    "# initialize the criterion and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0528ba",
   "metadata": {},
   "source": [
    "## Get features partition and cluster\n",
    "- Load the model \n",
    "- Run it on the training data and partition it into 4 different sets for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04e91c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0b006da2df43d8a1e23e8d4819fbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools  # for cartesian product for loops\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# while looping through the batch, save a dictionary (where the position of the list match up):\n",
    "# {\n",
    "#     activations : [],  # the features\n",
    "#     labels : [],       # the corresponding labels \n",
    "#     correctly_classified : [], # whether the given model correctly classified\n",
    "#     groups : [], # is this cheating? \n",
    "# }\n",
    "\n",
    "\n",
    "def get_save_features_hook_fn(list_for_features: [torch.Tensor]):\n",
    "    \"\"\"\n",
    "        Returns:\n",
    "            function for forward hook that appends the input (i.e. features) of the last layer to list_for_features\n",
    "    \"\"\"\n",
    "    def hook_fn(self, inp, outp):\n",
    "        list_for_features.append(inp[0].detach().cpu())  # this saves the feature to the given feature list\n",
    "    return hook_fn\n",
    "    \n",
    "\n",
    "def get_output_sets(loader, model, classifying_groups = False):\n",
    "    output_sets = {'activations' : [],  # the features\n",
    "                   'predicted' : [], # the model's prediction\n",
    "#                    'groups' : [], # is this cheating? \n",
    "                   'labels' : [],       # the corresponding labels \n",
    "                   'idx' : []\n",
    "                 }\n",
    "    model.eval()\n",
    "    # the line below makes it so that whenever model.forward is called, we save the features to our activations list.\n",
    "    # the feature handler ensures we remove the hook after we are done extracting the features\n",
    "    feature_handler = model.fc.register_forward_hook(get_save_features_hook_fn(output_sets['activations']))\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch_idx, batch in enumerate(tqdm(loader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            x, y, g, idx = batch\n",
    "            outputs = model(x)  \n",
    "            to_predict = g if classifying_groups else y\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            output_sets['labels'].append(y.detach().cpu())\n",
    "            output_sets['predicted'].append(predicted.detach().cpu())\n",
    "            output_sets['idx'].append(idx.detach().cpu())\n",
    "    \n",
    "    feature_handler.remove()  # we unregister the forward hook. \n",
    "    return output_sets\n",
    "\n",
    "\n",
    "output_dict_save_path = os.path.join(log_path,  'output_sets.pth')\n",
    "\n",
    "# # save output_sets\n",
    "output_sets = get_output_sets(train_loader, model)\n",
    "torch.save(output_sets, output_dict_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f213a8",
   "metadata": {},
   "source": [
    "## Cluster the sets\n",
    "- First we load the output sets again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f568de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272 torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "group_features_dict = {}  # this contains the features of the different labels/correctly-classified groups\n",
    "group_idx_dict = {}\n",
    "output_sets = torch.load(output_dict_save_path)\n",
    "\n",
    "print(len(output_sets['activations']), output_sets['activations'][0].shape)\n",
    "x_feature = torch.cat(output_sets['activations'])\n",
    "y_array = torch.cat(output_sets['labels'])\n",
    "predicted = torch.cat(output_sets['predicted'])\n",
    "idxs = torch.cat(output_sets['idx'])\n",
    "\n",
    "# then we partition it into the combinations: label x [correctly, incorrectly classified]\n",
    "for label in range(n_classes):\n",
    "    correct_select = (y_array == label) & (y_array == predicted)\n",
    "    group_features_dict[f'class{label}_correct'] = x_feature[correct_select]\n",
    "    group_idx_dict[f'class{label}_correct'] = idxs[correct_select].numpy()\n",
    "    \n",
    "    wrong_select = (y_array == label) & (y_array != predicted)\n",
    "    group_features_dict[f'class{label}_wrong'] = x_feature[wrong_select]\n",
    "    group_idx_dict[f'class{label}_wrong'] = idxs[wrong_select].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051ff73",
   "metadata": {},
   "source": [
    "- Cluster the subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73810cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_groups =  8\n"
     ]
    }
   ],
   "source": [
    "from copy import copy  # copy the labels from the cluster\n",
    "n_clusters = 2  # 4, 300: 781, \n",
    "max_iter = 300  # 5, 300: 535.927, 400: \n",
    "\n",
    "\n",
    "def get_idxs_to_subgroup_labels(cluster_model, n_clusters, group_features_dict, n_classes, group_idx_dict, max_iter=300):\n",
    "    cluster_assignments = {}\n",
    "    for label in range(n_classes):\n",
    "        for stat in ['correct', 'wrong']:\n",
    "            cluster_model = KMeans(n_clusters, max_iter=max_iter)\n",
    "            cluster_model.fit(group_features_dict[f'class{label}_{stat}'])\n",
    "            cluster_assignments[f'class{label}_{stat}'] = copy(cluster_model.labels_)\n",
    "\n",
    "\n",
    "    # Now we map each idx for each point to its appropriate group label\n",
    "    idxs_to_subgroup_labels = {}\n",
    "\n",
    "    group_label_counter = 0  # so that the group labels for each groups get a unique number\n",
    "    for k in cluster_assignments.keys():\n",
    "        idx_array = group_idx_dict[k]\n",
    "        assignment_array = cluster_assignments[k] + group_label_counter\n",
    "        idxs_to_subgroup_labels.update({idx: assignment for idx, assignment in zip(idx_array, assignment_array)})\n",
    "        group_label_counter += len(np.unique(cluster_assignments[k]))\n",
    "\n",
    "    print(\"n_groups = \", group_label_counter)\n",
    "    return idxs_to_subgroup_labels, group_label_counter\n",
    "\n",
    "idxs_to_subgroup_labels, group_label_counter = get_idxs_to_subgroup_labels(cluster_model, n_clusters, group_features_dict, n_classes, group_idx_dict, max_iter)\n",
    "torch.save(idxs_to_subgroup_labels, os.path.join(log_sr_path, 'idx_to_subgroup_labels.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65957fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.pseudogrouplabels_dataset import PseudoGroupLabelsDataset\n",
    "\n",
    "# PseudoGroupLabelsDataset implements so that the dataloader \n",
    "pgl_train_data = PseudoGroupLabelsDataset(train_data, idxs_to_subgroup_labels, group_label_counter)\n",
    "pgl_train_loader = pgl_train_data.get_loader(train=True, reweight_groups=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6cbdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.get_g(355)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5fca3",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd77fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_to_pseudogroup = {i:{sg: 0 for sg in range(group_label_counter)} for i in range(4)}  # dict of {real_group: {pseudo_group_label: count}}\n",
    "pseudogroup_to_group = {sg: {i: 0 for i in range(4)} for sg in range(group_label_counter)}\n",
    "\n",
    "for idx, sgl in idxs_to_subgroup_labels.items():\n",
    "    group_to_pseudogroup[train_data.get_g(idx)][sgl] += 1\n",
    "    pseudogroup_to_group[sgl][train_data.get_g(idx)] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df48cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groups to pseudogroups\n",
      "{0: {0: 61904, 1: 9504, 2: 69, 3: 152, 4: 0, 5: 0, 6: 0, 7: 0},\n",
      " 1: {0: 62342, 1: 4520, 2: 2, 3: 10, 4: 0, 5: 0, 6: 0, 7: 0},\n",
      " 2: {0: 0, 1: 0, 2: 0, 3: 0, 4: 4401, 5: 2549, 6: 9546, 7: 6384},\n",
      " 3: {0: 0, 1: 0, 2: 0, 3: 0, 4: 28, 5: 6, 6: 369, 7: 984}}\n",
      "pseudogroups to groups\n",
      "{0: {0: 61904, 1: 62342, 2: 0, 3: 0},\n",
      " 1: {0: 9504, 1: 4520, 2: 0, 3: 0},\n",
      " 2: {0: 69, 1: 2, 2: 0, 3: 0},\n",
      " 3: {0: 152, 1: 10, 2: 0, 3: 0},\n",
      " 4: {0: 0, 1: 0, 2: 4401, 3: 28},\n",
      " 5: {0: 0, 1: 0, 2: 2549, 3: 6},\n",
      " 6: {0: 0, 1: 0, 2: 9546, 3: 369},\n",
      " 7: {0: 0, 1: 0, 2: 6384, 3: 984}}\n",
      "pseudogroups: \n",
      "\t0,1:\tLabel0_correct\n",
      "\t2,3:\tLabel0_wrong\n",
      "\t4,5:\tLabel1_correct\n",
      "\t6,7:\tLabel1_wrong\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(\"groups to pseudogroups\")\n",
    "pprint(group_to_pseudogroup)\n",
    "print('pseudogroups to groups')\n",
    "pprint(pseudogroup_to_group)\n",
    "print(\"pseudogroups: \\n\\t0,1:\\tLabel0_correct\\n\\t2,3:\\tLabel0_wrong\\n\\t4,5:\\tLabel1_correct\\n\\t6,7:\\tLabel1_wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c61b3",
   "metadata": {},
   "source": [
    "## Finally retrain with gDRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515fb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import LossComputer\n",
    "from train import run_epoch \n",
    "import csv\n",
    "\n",
    "class BasicLogger:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def write(self, msg):\n",
    "        print(msg)\n",
    "        \n",
    "robust = True\n",
    "\n",
    "logger = BasicLogger()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "criterion.to(device)\n",
    "loss_computer = LossComputer(criterion, is_robust=robust, dataset=pgl_train_data)\n",
    "wd = 0.0001\n",
    "\n",
    "######################################################\n",
    "###########        Load models and optimizers.  ######\n",
    "######################################################\n",
    "model = resnet10vw(resnet_width, num_classes=n_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=wd)\n",
    "\n",
    "######################################################\n",
    "###########          Log files                  ######\n",
    "######################################################\n",
    "mode = 'w'\n",
    "log_dir = os.path.join(log_sr_path, 'cluster_gDRO')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "train_path = open(os.path.join(log_dir, 'train.csv'), mode)\n",
    "val_path = open(os.path.join(log_dir, 'val.csv'), mode)\n",
    "\n",
    "n_groups = pgl_train_data.n_groups\n",
    "val_n_groups = 4\n",
    "total_acc_per_group = [f'total_acc:g{i}' for i in range(n_groups)]\n",
    "group_accs = [f'group{i}_acc' for i in range(n_groups)]\n",
    "group_margins = [f'group{i}_margin' for i in range(n_groups)]\n",
    "train_columns = ['epoch', 'total_acc',  'split_acc', 'loss',\n",
    "                 'avg_margin'] + total_acc_per_group + group_accs + group_margins\n",
    "total_acc_per_group = [f'total_acc:g{i}' for i in range(val_n_groups)]\n",
    "group_accs = [f'group{i}_acc' for i in range(val_n_groups)]\n",
    "group_margins = [f'group{i}_margin' for i in range(val_n_groups)]\n",
    "valtest_columns = ['epoch', 'total_acc',  'split_acc',\n",
    "                       'avg_margin'] + total_acc_per_group + group_accs + group_margins\n",
    "\n",
    "train_writer = csv.DictWriter(train_path, fieldnames=train_columns)\n",
    "val_writer = csv.DictWriter(val_path, fieldnames=valtest_columns)\n",
    "train_writer.writeheader()\n",
    "val_writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479eef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af123afb8a44d65a1f5e64d1cddb2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.524, avg_margin: -0.081. adv_probs: tensor([0.1689, 0.2455, 0.0489, 0.0550, 0.1050, 0.0911, 0.1256, 0.1599],\n",
      "       device='cuda:0')\n",
      "[1,   400] loss: 0.566, avg_margin: 0.077. adv_probs: tensor([0.1207, 0.3461, 0.0186, 0.0227, 0.0691, 0.0486, 0.1146, 0.2597],\n",
      "       device='cuda:0')\n",
      "[1,   600] loss: 0.608, avg_margin: 0.118. adv_probs: tensor([0.0858, 0.4021, 0.0060, 0.0081, 0.0469, 0.0251, 0.0980, 0.3280],\n",
      "       device='cuda:0')\n",
      "[1,   800] loss: 0.626, avg_margin: 0.118. adv_probs: tensor([0.0687, 0.4272, 0.0020, 0.0029, 0.0355, 0.0152, 0.0915, 0.3572],\n",
      "       device='cuda:0')\n",
      "[1,  1000] loss: 0.629, avg_margin: 0.112. adv_probs: tensor([0.0586, 0.4382, 0.0006, 0.0011, 0.0289, 0.0093, 0.0876, 0.3757],\n",
      "       device='cuda:0')\n",
      "[1,  1200] loss: 0.622, avg_margin: 0.107. adv_probs: tensor([5.1335e-02, 4.4440e-01, 2.0637e-04, 3.7226e-04, 2.4104e-02, 5.8176e-03,\n",
      "        8.2343e-02, 3.9142e-01], device='cuda:0')\n",
      "\n",
      "{'avg_margin': '0.1040',\n",
      " 'epoch': 1,\n",
      " 'group0_acc': '0.7640',\n",
      " 'group0_margin': '0.1317',\n",
      " 'group1_acc': '0.5615',\n",
      " 'group1_margin': '-0.1222',\n",
      " 'group2_acc': '0.1549',\n",
      " 'group2_margin': '-0.6734',\n",
      " 'group3_acc': '0.2840',\n",
      " 'group3_margin': '-0.4244',\n",
      " 'group4_acc': '0.8234',\n",
      " 'group4_margin': '0.2559',\n",
      " 'group5_acc': '0.9049',\n",
      " 'group5_margin': '0.4063',\n",
      " 'group6_acc': '0.7209',\n",
      " 'group6_margin': '0.1001',\n",
      " 'group7_acc': '0.5429',\n",
      " 'group7_margin': '-0.1042',\n",
      " 'loss': '0.2262',\n",
      " 'split_acc': '0.7370',\n",
      " 'total_acc': '0.7370',\n",
      " 'total_acc:g0': '0.7640',\n",
      " 'total_acc:g1': '0.5615',\n",
      " 'total_acc:g2': '0.1549',\n",
      " 'total_acc:g3': '0.2840',\n",
      " 'total_acc:g4': '0.8234',\n",
      " 'total_acc:g5': '0.9049',\n",
      " 'total_acc:g6': '0.7209',\n",
      " 'total_acc:g7': '0.5429'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffdf38b52a345caa1ecfc81ab502339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'avg_margin': '0.0726',\n",
      " 'epoch': 0,\n",
      " 'group0_acc': '0.7058',\n",
      " 'group0_margin': '0.0549',\n",
      " 'group1_acc': '0.6841',\n",
      " 'group1_margin': '0.0442',\n",
      " 'group2_acc': '0.7234',\n",
      " 'group2_margin': '0.2152',\n",
      " 'group3_acc': '0.5000',\n",
      " 'group3_margin': '-0.0619',\n",
      " 'split_acc': '0.6974',\n",
      " 'total_acc': '0.6974',\n",
      " 'total_acc:g0': '0.7058',\n",
      " 'total_acc:g1': '0.6841',\n",
      " 'total_acc:g2': '0.7234',\n",
      " 'total_acc:g3': '0.5000'}\n",
      "Train epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4809c97fdd164db5ab4a43ca29bb8009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   200] loss: 0.634, avg_margin: 0.003. adv_probs: tensor([4.8634e-02, 4.4819e-01, 4.1912e-05, 8.4238e-05, 1.8769e-02, 3.3075e-03,\n",
      "        8.2048e-02, 3.9893e-01], device='cuda:0')\n",
      "[2,   400] loss: 0.624, avg_margin: 0.004. adv_probs: tensor([4.8999e-02, 4.4877e-01, 1.4149e-05, 2.9624e-05, 1.6416e-02, 2.4464e-03,\n",
      "        8.2678e-02, 4.0065e-01], device='cuda:0')\n",
      "[2,   600] loss: 0.626, avg_margin: 0.014. adv_probs: tensor([4.6417e-02, 4.5010e-01, 4.3421e-06, 9.9370e-06, 1.5692e-02, 1.7681e-03,\n",
      "        8.3443e-02, 4.0256e-01], device='cuda:0')\n",
      "[2,   800] loss: 0.610, avg_margin: 0.022. adv_probs: tensor([4.4866e-02, 4.5120e-01, 1.4053e-06, 3.4755e-06, 1.4234e-02, 1.2377e-03,\n",
      "        8.6231e-02, 4.0223e-01], device='cuda:0')\n",
      "[2,  1000] loss: 0.613, avg_margin: 0.025. adv_probs: tensor([4.3483e-02, 4.5325e-01, 4.3567e-07, 1.1681e-06, 1.3672e-02, 8.5380e-04,\n",
      "        8.8704e-02, 4.0003e-01], device='cuda:0')\n",
      "[2,  1200] loss: 0.615, avg_margin: 0.036. adv_probs: tensor([3.8636e-02, 4.5963e-01, 1.4209e-07, 4.3131e-07, 1.2566e-02, 5.4595e-04,\n",
      "        8.7833e-02, 4.0079e-01], device='cuda:0')\n",
      "\n",
      "{'avg_margin': '0.0384',\n",
      " 'epoch': 2,\n",
      " 'group0_acc': '0.6708',\n",
      " 'group0_margin': '0.0426',\n",
      " 'group1_acc': '0.6839',\n",
      " 'group1_margin': '-0.0090',\n",
      " 'group2_acc': '0.3662',\n",
      " 'group2_margin': '-0.4367',\n",
      " 'group3_acc': '0.5741',\n",
      " 'group3_margin': '-0.1599',\n",
      " 'group4_acc': '0.6494',\n",
      " 'group4_margin': '0.0880',\n",
      " 'group5_acc': '0.7562',\n",
      " 'group5_margin': '0.2301',\n",
      " 'group6_acc': '0.6045',\n",
      " 'group6_margin': '0.0216',\n",
      " 'group7_acc': '0.6246',\n",
      " 'group7_margin': '-0.0078',\n",
      " 'loss': '0.2200',\n",
      " 'split_acc': '0.6663',\n",
      " 'total_acc': '0.6663',\n",
      " 'total_acc:g0': '0.6708',\n",
      " 'total_acc:g1': '0.6839',\n",
      " 'total_acc:g2': '0.3662',\n",
      " 'total_acc:g3': '0.5741',\n",
      " 'total_acc:g4': '0.6494',\n",
      " 'total_acc:g5': '0.7562',\n",
      " 'total_acc:g6': '0.6045',\n",
      " 'total_acc:g7': '0.6246'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1527b9e4249e4124ba3f7bf7e96ee562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'avg_margin': '0.2489',\n",
      " 'epoch': 1,\n",
      " 'group0_acc': '0.8101',\n",
      " 'group0_margin': '0.2693',\n",
      " 'group1_acc': '0.8358',\n",
      " 'group1_margin': '0.2890',\n",
      " 'group2_acc': '0.6900',\n",
      " 'group2_margin': '0.1114',\n",
      " 'group3_acc': '0.3846',\n",
      " 'group3_margin': '-0.3595',\n",
      " 'split_acc': '0.7995',\n",
      " 'total_acc': '0.7995',\n",
      " 'total_acc:g0': '0.8101',\n",
      " 'total_acc:g1': '0.8358',\n",
      " 'total_acc:g2': '0.6900',\n",
      " 'total_acc:g3': '0.3846'}\n",
      "Train epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e7c28b3daf434d9a0205de7ae951f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,   200] loss: 0.605, avg_margin: 0.100. adv_probs: tensor([3.3321e-02, 4.6369e-01, 3.3071e-08, 1.0899e-07, 9.6964e-03, 3.1633e-04,\n",
      "        8.6839e-02, 4.0614e-01], device='cuda:0')\n",
      "[3,   400] loss: 0.603, avg_margin: 0.074. adv_probs: tensor([3.2709e-02, 4.6592e-01, 1.0824e-08, 3.9180e-08, 9.7871e-03, 2.3403e-04,\n",
      "        9.0592e-02, 4.0076e-01], device='cuda:0')\n",
      "[3,   600] loss: 0.591, avg_margin: 0.082. adv_probs: tensor([3.1163e-02, 4.6702e-01, 3.6785e-09, 1.4132e-08, 8.6278e-03, 1.5406e-04,\n",
      "        9.4334e-02, 3.9870e-01], device='cuda:0')\n",
      "[3,   800] loss: 0.590, avg_margin: 0.086. adv_probs: tensor([2.9399e-02, 4.6900e-01, 1.2055e-09, 5.0048e-09, 7.5135e-03, 1.0158e-04,\n",
      "        9.6213e-02, 3.9777e-01], device='cuda:0')\n",
      "[3,  1000] loss: 0.592, avg_margin: 0.087. adv_probs: tensor([2.7891e-02, 4.7029e-01, 4.1564e-10, 1.8717e-09, 6.3696e-03, 6.7708e-05,\n",
      "        9.9070e-02, 3.9632e-01], device='cuda:0')\n",
      "[3,  1200] loss: 0.610, avg_margin: 0.085. adv_probs: tensor([2.5946e-02, 4.7321e-01, 1.3231e-10, 6.3797e-10, 5.4458e-03, 4.1745e-05,\n",
      "        1.0043e-01, 3.9492e-01], device='cuda:0')\n",
      "\n",
      "{'avg_margin': '0.0879',\n",
      " 'epoch': 3,\n",
      " 'group0_acc': '0.7000',\n",
      " 'group0_margin': '0.0925',\n",
      " 'group1_acc': '0.6959',\n",
      " 'group1_margin': '0.0299',\n",
      " 'group2_acc': '0.3521',\n",
      " 'group2_margin': '-0.4949',\n",
      " 'group3_acc': '0.5679',\n",
      " 'group3_margin': '-0.1741',\n",
      " 'group4_acc': '0.6927',\n",
      " 'group4_margin': '0.1722',\n",
      " 'group5_acc': '0.7965',\n",
      " 'group5_margin': '0.3311',\n",
      " 'group6_acc': '0.6338',\n",
      " 'group6_margin': '0.0636',\n",
      " 'group7_acc': '0.6520',\n",
      " 'group7_margin': '0.0312',\n",
      " 'loss': '0.2151',\n",
      " 'split_acc': '0.6945',\n",
      " 'total_acc': '0.6945',\n",
      " 'total_acc:g0': '0.7000',\n",
      " 'total_acc:g1': '0.6959',\n",
      " 'total_acc:g2': '0.3521',\n",
      " 'total_acc:g3': '0.5679',\n",
      " 'total_acc:g4': '0.6927',\n",
      " 'total_acc:g5': '0.7965',\n",
      " 'total_acc:g6': '0.6338',\n",
      " 'total_acc:g7': '0.6520'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775fb4bf00d047e2b3456a4c315ba11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'avg_margin': '0.0090',\n",
      " 'epoch': 2,\n",
      " 'group0_acc': '0.6384',\n",
      " 'group0_margin': '-0.0055',\n",
      " 'group1_acc': '0.6385',\n",
      " 'group1_margin': '0.0090',\n",
      " 'group2_acc': '0.6708',\n",
      " 'group2_margin': '0.0677',\n",
      " 'group3_acc': '0.4670',\n",
      " 'group3_margin': '-0.2401',\n",
      " 'split_acc': '0.6416',\n",
      " 'total_acc': '0.6416',\n",
      " 'total_acc:g0': '0.6384',\n",
      " 'total_acc:g1': '0.6385',\n",
      " 'total_acc:g2': '0.6708',\n",
      " 'total_acc:g3': '0.4670'}\n",
      "Train epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9b9dd75b8c485fa9da0a966f141bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,   200] loss: 0.588, avg_margin: 0.074. adv_probs: tensor([2.4224e-02, 4.7343e-01, 2.9336e-11, 1.6063e-10, 4.5277e-03, 2.2510e-05,\n",
      "        1.0444e-01, 3.9336e-01], device='cuda:0')\n",
      "[4,   400] loss: 0.576, avg_margin: 0.085. adv_probs: tensor([2.3854e-02, 4.7317e-01, 9.8072e-12, 5.8204e-11, 4.2773e-03, 1.6649e-05,\n",
      "        1.0607e-01, 3.9261e-01], device='cuda:0')\n",
      "[4,   600] loss: 0.586, avg_margin: 0.096. adv_probs: tensor([2.2432e-02, 4.7764e-01, 3.4801e-12, 2.2347e-11, 3.3526e-03, 1.0624e-05,\n",
      "        1.0453e-01, 3.9204e-01], device='cuda:0')\n",
      "[4,   800] loss: 0.592, avg_margin: 0.100. adv_probs: tensor([2.1318e-02, 4.7454e-01, 1.2651e-12, 8.0543e-12, 2.6148e-03, 6.1267e-06,\n",
      "        1.0087e-01, 4.0065e-01], device='cuda:0')\n",
      "[4,  1000] loss: 0.589, avg_margin: 0.094. adv_probs: tensor([2.1387e-02, 4.7669e-01, 4.3282e-13, 3.3177e-12, 2.1611e-03, 4.0020e-06,\n",
      "        9.8867e-02, 4.0089e-01], device='cuda:0')\n",
      "[4,  1200] loss: 0.589, avg_margin: 0.091. adv_probs: tensor([2.1174e-02, 4.7736e-01, 1.4599e-13, 1.1580e-12, 1.8402e-03, 2.5934e-06,\n",
      "        9.8282e-02, 4.0135e-01], device='cuda:0')\n",
      "\n",
      "{'avg_margin': '0.0938',\n",
      " 'epoch': 4,\n",
      " 'group0_acc': '0.6942',\n",
      " 'group0_margin': '0.0875',\n",
      " 'group1_acc': '0.7057',\n",
      " 'group1_margin': '0.0702',\n",
      " 'group2_acc': '0.4225',\n",
      " 'group2_margin': '-0.4393',\n",
      " 'group3_acc': '0.5679',\n",
      " 'group3_margin': '-0.1371',\n",
      " 'group4_acc': '0.7273',\n",
      " 'group4_margin': '0.2264',\n",
      " 'group5_acc': '0.8180',\n",
      " 'group5_margin': '0.3952',\n",
      " 'group6_acc': '0.6576',\n",
      " 'group6_margin': '0.1048',\n",
      " 'group7_acc': '0.6762',\n",
      " 'group7_margin': '0.0549',\n",
      " 'loss': '0.2003',\n",
      " 'split_acc': '0.6947',\n",
      " 'total_acc': '0.6947',\n",
      " 'total_acc:g0': '0.6942',\n",
      " 'total_acc:g1': '0.7057',\n",
      " 'total_acc:g2': '0.4225',\n",
      " 'total_acc:g3': '0.5679',\n",
      " 'total_acc:g4': '0.7273',\n",
      " 'total_acc:g5': '0.8180',\n",
      " 'total_acc:g6': '0.6576',\n",
      " 'total_acc:g7': '0.6762'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f9d7dc826749679197167026cd1c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'avg_margin': '0.1074',\n",
      " 'epoch': 3,\n",
      " 'group0_acc': '0.6676',\n",
      " 'group0_margin': '0.0720',\n",
      " 'group1_acc': '0.6938',\n",
      " 'group1_margin': '0.1401',\n",
      " 'group2_acc': '0.6910',\n",
      " 'group2_margin': '0.1451',\n",
      " 'group3_acc': '0.4560',\n",
      " 'group3_margin': '-0.3134',\n",
      " 'split_acc': '0.6800',\n",
      " 'total_acc': '0.6800',\n",
      " 'total_acc:g0': '0.6676',\n",
      " 'total_acc:g1': '0.6938',\n",
      " 'total_acc:g2': '0.6910',\n",
      " 'total_acc:g3': '0.4560'}\n",
      "Train epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a901ff759b4346aa45692751bccb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,   200] loss: 0.580, avg_margin: 0.132. adv_probs: tensor([1.9869e-02, 4.8016e-01, 3.7669e-14, 3.1826e-13, 1.3725e-03, 1.3710e-06,\n",
      "        9.6347e-02, 4.0225e-01], device='cuda:0')\n",
      "[5,   400] loss: 0.570, avg_margin: 0.110. adv_probs: tensor([2.0422e-02, 4.8246e-01, 1.3646e-14, 1.1774e-13, 1.2561e-03, 9.3779e-07,\n",
      "        1.0460e-01, 3.9126e-01], device='cuda:0')\n",
      "[5,   600] loss: 0.572, avg_margin: 0.129. adv_probs: tensor([1.8421e-02, 4.7883e-01, 4.8816e-15, 4.8164e-14, 9.8565e-04, 6.1071e-07,\n",
      "        9.8808e-02, 4.0295e-01], device='cuda:0')\n",
      "[5,   800] loss: 0.567, avg_margin: 0.132. adv_probs: tensor([1.7601e-02, 4.8241e-01, 1.6519e-15, 1.8253e-14, 8.1975e-04, 3.8425e-07,\n",
      "        9.9456e-02, 3.9972e-01], device='cuda:0')\n",
      "[5,  1000] loss: 0.550, avg_margin: 0.132. adv_probs: tensor([1.8061e-02, 4.8146e-01, 6.2096e-16, 7.2589e-15, 6.2605e-04, 2.1990e-07,\n",
      "        9.6834e-02, 4.0302e-01], device='cuda:0')\n",
      "[5,  1200] loss: 0.561, avg_margin: 0.137. adv_probs: tensor([1.7214e-02, 4.8336e-01, 2.2178e-16, 2.8395e-15, 5.3687e-04, 1.4078e-07,\n",
      "        9.8557e-02, 4.0034e-01], device='cuda:0')\n",
      "\n",
      "{'avg_margin': '0.1387',\n",
      " 'epoch': 5,\n",
      " 'group0_acc': '0.7175',\n",
      " 'group0_margin': '0.1317',\n",
      " 'group1_acc': '0.7079',\n",
      " 'group1_margin': '0.1075',\n",
      " 'group2_acc': '0.3662',\n",
      " 'group2_margin': '-0.5105',\n",
      " 'group3_acc': '0.5494',\n",
      " 'group3_margin': '-0.1636',\n",
      " 'group4_acc': '0.7580',\n",
      " 'group4_margin': '0.3079',\n",
      " 'group5_acc': '0.8470',\n",
      " 'group5_margin': '0.4846',\n",
      " 'group6_acc': '0.6848',\n",
      " 'group6_margin': '0.1491',\n",
      " 'group7_acc': '0.6948',\n",
      " 'group7_margin': '0.0922',\n",
      " 'loss': '0.2044',\n",
      " 'split_acc': '0.7165',\n",
      " 'total_acc': '0.7165',\n",
      " 'total_acc:g0': '0.7175',\n",
      " 'total_acc:g1': '0.7079',\n",
      " 'total_acc:g2': '0.3662',\n",
      " 'total_acc:g3': '0.5494',\n",
      " 'total_acc:g4': '0.7580',\n",
      " 'total_acc:g5': '0.8470',\n",
      " 'total_acc:g6': '0.6848',\n",
      " 'total_acc:g7': '0.6948'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e6ceba63cc4cad82dca7794bb797cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'avg_margin': '0.0849',\n",
      " 'epoch': 4,\n",
      " 'group0_acc': '0.6586',\n",
      " 'group0_margin': '0.0737',\n",
      " 'group1_acc': '0.6624',\n",
      " 'group1_margin': '0.1352',\n",
      " 'group2_acc': '0.6058',\n",
      " 'group2_margin': '0.0017',\n",
      " 'group3_acc': '0.4341',\n",
      " 'group3_margin': '-0.3692',\n",
      " 'split_acc': '0.6505',\n",
      " 'total_acc': '0.6505',\n",
      " 'total_acc:g0': '0.6586',\n",
      " 'total_acc:g1': '0.6624',\n",
      " 'total_acc:g2': '0.6058',\n",
      " 'total_acc:g3': '0.4341'}\n",
      "Train epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d33a16feb54362ac41e15799a94deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,   200] loss: 0.537, avg_margin: 0.221. adv_probs: tensor([1.5881e-02, 4.8686e-01, 5.9590e-17, 7.4782e-16, 3.9768e-04, 6.8503e-08,\n",
      "        9.6398e-02, 4.0046e-01], device='cuda:0')\n",
      "[6,   400] loss: 0.555, avg_margin: 0.195. adv_probs: tensor([1.5396e-02, 4.8234e-01, 2.2846e-17, 2.7650e-16, 3.2736e-04, 4.2513e-08,\n",
      "        9.4787e-02, 4.0715e-01], device='cuda:0')\n",
      "[6,   600] loss: 0.554, avg_margin: 0.174. adv_probs: tensor([1.5858e-02, 4.8123e-01, 8.1436e-18, 1.1158e-16, 2.8064e-04, 2.7520e-08,\n",
      "        9.6101e-02, 4.0653e-01], device='cuda:0')\n",
      "[6,   800] loss: 0.555, avg_margin: 0.174. adv_probs: tensor([1.5184e-02, 4.8497e-01, 2.9766e-18, 4.3809e-17, 2.1098e-04, 1.5659e-08,\n",
      "        9.0830e-02, 4.0881e-01], device='cuda:0')\n",
      "[6,  1000] loss: 0.551, avg_margin: 0.169. adv_probs: tensor([1.5711e-02, 4.8570e-01, 1.0572e-18, 1.8258e-17, 1.7349e-04, 1.0672e-08,\n",
      "        9.2163e-02, 4.0625e-01], device='cuda:0')\n",
      "[6,  1200] loss: 0.545, avg_margin: 0.168. adv_probs: tensor([1.5677e-02, 4.8314e-01, 4.1950e-19, 7.5458e-18, 1.5620e-04, 7.5420e-09,\n",
      "        9.5439e-02, 4.0559e-01], device='cuda:0')\n",
      "\n",
      "{'avg_margin': '0.1682',\n",
      " 'epoch': 6,\n",
      " 'group0_acc': '0.7233',\n",
      " 'group0_margin': '0.1551',\n",
      " 'group1_acc': '0.7240',\n",
      " 'group1_margin': '0.1535',\n",
      " 'group2_acc': '0.3944',\n",
      " 'group2_margin': '-0.5457',\n",
      " 'group3_acc': '0.6111',\n",
      " 'group3_margin': '-0.1574',\n",
      " 'group4_acc': '0.7668',\n",
      " 'group4_margin': '0.3592',\n",
      " 'group5_acc': '0.8552',\n",
      " 'group5_margin': '0.5536',\n",
      " 'group6_acc': '0.7038',\n",
      " 'group6_margin': '0.1997',\n",
      " 'group7_acc': '0.7128',\n",
      " 'group7_margin': '0.1406',\n",
      " 'loss': '0.2013',\n",
      " 'split_acc': '0.7247',\n",
      " 'total_acc': '0.7247',\n",
      " 'total_acc:g0': '0.7233',\n",
      " 'total_acc:g1': '0.7240',\n",
      " 'total_acc:g2': '0.3944',\n",
      " 'total_acc:g3': '0.6111',\n",
      " 'total_acc:g4': '0.7668',\n",
      " 'total_acc:g5': '0.8552',\n",
      " 'total_acc:g6': '0.7038',\n",
      " 'total_acc:g7': '0.7128'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45fca7dc75446f8a93f40c928235570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'avg_margin': '0.1913',\n",
      " 'epoch': 5,\n",
      " 'group0_acc': '0.7219',\n",
      " 'group0_margin': '0.1187',\n",
      " 'group1_acc': '0.7823',\n",
      " 'group1_margin': '0.2537',\n",
      " 'group2_acc': '0.7171',\n",
      " 'group2_margin': '0.2536',\n",
      " 'group3_acc': '0.5055',\n",
      " 'group3_margin': '-0.2241',\n",
      " 'split_acc': '0.7443',\n",
      " 'total_acc': '0.7443',\n",
      " 'total_acc:g0': '0.7219',\n",
      " 'total_acc:g1': '0.7823',\n",
      " 'total_acc:g2': '0.7171',\n",
      " 'total_acc:g3': '0.5055'}\n",
      "Train epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed54b6c5ce1247afbcc060438f37d3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,   200] loss: 0.529, avg_margin: 0.194. adv_probs: tensor([1.5469e-02, 4.8466e-01, 1.0961e-19, 2.3789e-18, 1.1645e-04, 4.3554e-09,\n",
      "        1.0269e-01, 3.9707e-01], device='cuda:0')\n",
      "[7,   400] loss: 0.541, avg_margin: 0.204. adv_probs: tensor([1.4714e-02, 4.8835e-01, 4.1437e-20, 1.0857e-18, 9.3078e-05, 2.7317e-09,\n",
      "        1.0101e-01, 3.9584e-01], device='cuda:0')\n",
      "[7,   600] loss: 0.534, avg_margin: 0.223. adv_probs: tensor([1.3354e-02, 4.8595e-01, 1.5821e-20, 4.3013e-19, 7.6210e-05, 1.5429e-09,\n",
      "        1.0432e-01, 3.9630e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    # train\n",
    "    logger.write(f'Train epoch {epoch}')\n",
    "    run_epoch(epoch + 1, model, device, optimizer, pgl_train_loader, loss_computer, train_writer, logger,\n",
    "              is_training=True, is_robust=robust, classifying_groups=False)\n",
    "    run_epoch(epoch, model, device, optimizer, val_loader, loss_computer, val_writer, logger,\n",
    "                  is_training=False, is_robust=True, classifying_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f00f7",
   "metadata": {},
   "source": [
    "# PLAYGROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking whether the number between saving and loading lists above is the same as brute force\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# model = model_sr\n",
    "# model.eval()\n",
    "\n",
    "# test_dict = {f'class{label}_{stat}':[] for label,stat in itertools.product(range(n_classes), ['correct', 'wrong'])}\n",
    "\n",
    "# with torch.set_grad_enabled(False):\n",
    "#     for batch_idx, batch in enumerate(tqdm(loader)):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         x, y, g, idx = batch\n",
    "#         outputs = model(x)  \n",
    "#         to_predict = y\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "#         for label in range(n_classes):\n",
    "#             test_dict[f'class{label}_correct'] += [x[(to_predict == predicted) & (y == label)].cpu().numpy()]\n",
    "#             test_dict[f'class{label}_wrong'] += [x[(to_predict != predicted) & (y == label)].cpu().numpy()]\n",
    "\n",
    "\n",
    "# for key in test_dict:\n",
    "#     print(key, np.concatenate(test_dict[key], axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3158a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# output_sets = {'activations' : [],  # the features\n",
    "#                'predicted' : [], # the model's prediction\n",
    "#                'labels' : [],       # the corresponding labels \n",
    "#                'idx' : []\n",
    "#              }\n",
    "# # the line below makes it so that whenever model.forward is called, we save the features to our activations list.\n",
    "# # the feature handler ensures we remove the hook after we are done extracting the features\n",
    "# feature_handler = model.fc.register_forward_hook(get_save_features_hook_fn(output_sets['activations']))\n",
    "# stop = 0\n",
    "# with torch.set_grad_enabled(False):\n",
    "#     for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         x, y, g, idx = batch\n",
    "\n",
    "#         outputs = model(x)  \n",
    "#         to_predict =  y\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         output_sets['labels'].append(y.cpu())\n",
    "#         output_sets['predicted'].append(predicted.cpu())\n",
    "#         output_sets['idx'].append(idx.cpu())\n",
    "#         print('x', x.norm())\n",
    "#         print('pred', predicted)\n",
    "#         print('y',y)\n",
    "#         print('idx', idx)\n",
    "#         print('----')\n",
    "#         break\n",
    "        \n",
    "    \n",
    "\n",
    "# # print()\n",
    "# # for k in output_sets:\n",
    "# #     if k == 'activations':\n",
    "# #         print(k, [output_sets[k][j].norm() for j in range(len(output_sets[k]))])\n",
    "# #     else:\n",
    "# #         print(k, output_sets[k])\n",
    "# #         print(torch.cat(output_sets[k]))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "686px",
    "left": "883px",
    "right": "20px",
    "top": "167px",
    "width": "445px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
